{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2843a79-d9d7-4159-a5cb-5ef276887f0a",
   "metadata": {},
   "source": [
    "***Advance usecases of PySpark***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6831ae9-ab82-4f2f-9323-24db7bb08f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DataType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e9e8b2e-6f47-4745-a243-bf54d71d2af3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://host.docker.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>sparkAdvanced</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2926663d400>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('sparkAdvanced').master('local[*]').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66b938b9-bc1f-4848-9e68-cf5d87ec338b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining schema for the dataset\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('Age', IntegerType(), nullable=True),\n",
    "    StructField('Gender', StringType(), nullable=True),\n",
    "    StructField('Occupation', StringType(), nullable=True),\n",
    "    StructField('SleepHours', FloatType(), nullable=True),\n",
    "    StructField('PhysicalActivity', FloatType(), nullable=True),\n",
    "    StructField('CaffeineIntake', IntegerType(), nullable=True),\n",
    "    StructField('AlcoholConsumption', IntegerType(), nullable=True),\n",
    "    StructField('Smoking', StringType(), nullable=True),\n",
    "    StructField('FamilyHistory', StringType(), nullable=True),\n",
    "    StructField('StressLevel', IntegerType(), nullable=True),\n",
    "    StructField('HeartRate', IntegerType(), nullable=True),\n",
    "    StructField('BreathingRate', IntegerType(), nullable=True),\n",
    "    StructField('SweatingLevel', IntegerType(), nullable=True),\n",
    "    StructField('Dizziness', StringType(), nullable=True),\n",
    "    StructField('Medication', StringType(), nullable=True),\n",
    "    StructField('TherapySessions', IntegerType(), nullable=True),\n",
    "    StructField('DietQuality', IntegerType(), nullable=True),\n",
    "    StructField('AnxietyLevel', IntegerType(), nullable=True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "298973d2-1874-48b2-8611-1882c502c31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+----------+----------------+--------------+------------------+-------+-------------+-----------+---------+-------------+-------------+---------+----------+---------------+-----------+------------+\n",
      "|Age|Gender|Occupation|SleepHours|PhysicalActivity|CaffeineIntake|AlcoholConsumption|Smoking|FamilyHistory|StressLevel|HeartRate|BreathingRate|SweatingLevel|Dizziness|Medication|TherapySessions|DietQuality|AnxietyLevel|\n",
      "+---+------+----------+----------+----------------+--------------+------------------+-------+-------------+-----------+---------+-------------+-------------+---------+----------+---------------+-----------+------------+\n",
      "| 29|Female|    Artist|       6.0|             2.7|           181|                10|    Yes|           No|         10|      114|           14|            4|       No|       Yes|              3|          7|           5|\n",
      "| 46| Other|     Nurse|       6.2|             5.7|           200|                 8|    Yes|          Yes|          1|       62|           23|            2|      Yes|        No|              2|          8|           3|\n",
      "+---+------+----------+----------+----------------+--------------+------------------+-------+-------------+-----------+---------+-------------+-------------+---------+----------+---------------+-----------+------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "main_df = spark.read.csv('socialanxiety/enhanced_anxiety_dataset.csv', schema=schema, header=True)\n",
    "main_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fbc80b5-3852-42c4-9a69-3e4b6eeef5d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Occupation: string (nullable = true)\n",
      " |-- SleepHours: float (nullable = true)\n",
      " |-- PhysicalActivity: float (nullable = true)\n",
      " |-- CaffeineIntake: integer (nullable = true)\n",
      " |-- AlcoholConsumption: integer (nullable = true)\n",
      " |-- Smoking: string (nullable = true)\n",
      " |-- FamilyHistory: string (nullable = true)\n",
      " |-- StressLevel: integer (nullable = true)\n",
      " |-- HeartRate: integer (nullable = true)\n",
      " |-- BreathingRate: integer (nullable = true)\n",
      " |-- SweatingLevel: integer (nullable = true)\n",
      " |-- Dizziness: string (nullable = true)\n",
      " |-- Medication: string (nullable = true)\n",
      " |-- TherapySessions: integer (nullable = true)\n",
      " |-- DietQuality: integer (nullable = true)\n",
      " |-- AnxietyLevel: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None,\n",
       " ['Age',\n",
       "  'Gender',\n",
       "  'Occupation',\n",
       "  'SleepHours',\n",
       "  'PhysicalActivity',\n",
       "  'CaffeineIntake',\n",
       "  'AlcoholConsumption',\n",
       "  'Smoking',\n",
       "  'FamilyHistory',\n",
       "  'StressLevel',\n",
       "  'HeartRate',\n",
       "  'BreathingRate',\n",
       "  'SweatingLevel',\n",
       "  'Dizziness',\n",
       "  'Medication',\n",
       "  'TherapySessions',\n",
       "  'DietQuality',\n",
       "  'AnxietyLevel'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_df.printSchema(), main_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a75b4009-c021-4323-b093-e36591855e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+----------------+------------------+-------+-------------+-----------+---------+-------------+-----------+------------+\n",
      "|Age|Gender|SleepHours|PhysicalActivity|AlcoholConsumption|Smoking|FamilyHistory|StressLevel|HeartRate|SweatingLevel|DietQuality|AnxietyLevel|\n",
      "+---+------+----------+----------------+------------------+-------+-------------+-----------+---------+-------------+-----------+------------+\n",
      "| 29|Female|       6.0|             2.7|                10|    Yes|           No|         10|      114|            4|          7|           5|\n",
      "| 46| Other|       6.2|             5.7|                 8|    Yes|          Yes|          1|       62|            2|          8|           3|\n",
      "| 64|  Male|       5.0|             3.7|                 4|     No|          Yes|          1|       91|            3|          1|           1|\n",
      "| 20|Female|       5.8|             2.8|                 6|    Yes|           No|          4|       86|            3|          1|           2|\n",
      "| 49|Female|       8.2|             2.3|                 4|    Yes|           No|          1|       98|            4|          3|           1|\n",
      "+---+------+----------+----------------+------------------+-------+-------------+-----------+---------+-------------+-----------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = main_df.select(['Age',\n",
    "  'Gender',\n",
    "  'SleepHours',\n",
    "  'PhysicalActivity',\n",
    "  'AlcoholConsumption',\n",
    "  'Smoking',\n",
    "  'FamilyHistory',\n",
    "  'StressLevel',\n",
    "  'HeartRate',\n",
    "  'SweatingLevel',\n",
    "  'DietQuality',\n",
    "  'AnxietyLevel'])\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa28afa-722d-4839-ae18-adf536f93997",
   "metadata": {},
   "source": [
    "*Note:* We can create dataframe out of user input by using **spark.createDataFrame(input_data, schema)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3160655e-3624-47b1-b607-2c3402c2bf80",
   "metadata": {},
   "source": [
    "***df.ccolect()***\n",
    "- We can retrieve data from spark RDD/DataFrame using collect.\n",
    "- It return data in a list of objects\n",
    "- NB: collect() should be used cautiously with large datasets because it can cause OutOfMemoryError on the driver. Alternatives like take() or show() are preferred for sampling or previewing large DataFrames.\n",
    "- docs: https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.collect.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d2b4d66-bb4d-4917-b5e7-91e2d6b1f6b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Age=29, Gender='Female', SleepHours=6.0, PhysicalActivity=2.700000047683716, AlcoholConsumption=10, Smoking='Yes', FamilyHistory='No', StressLevel=10, HeartRate=114, SweatingLevel=4, DietQuality=7, AnxietyLevel=5),\n",
       " Row(Age=46, Gender='Other', SleepHours=6.199999809265137, PhysicalActivity=5.699999809265137, AlcoholConsumption=8, Smoking='Yes', FamilyHistory='Yes', StressLevel=1, HeartRate=62, SweatingLevel=2, DietQuality=8, AnxietyLevel=3),\n",
       " Row(Age=64, Gender='Male', SleepHours=5.0, PhysicalActivity=3.700000047683716, AlcoholConsumption=4, Smoking='No', FamilyHistory='Yes', StressLevel=1, HeartRate=91, SweatingLevel=3, DietQuality=1, AnxietyLevel=1),\n",
       " Row(Age=20, Gender='Female', SleepHours=5.800000190734863, PhysicalActivity=2.799999952316284, AlcoholConsumption=6, Smoking='Yes', FamilyHistory='No', StressLevel=4, HeartRate=86, SweatingLevel=3, DietQuality=1, AnxietyLevel=2),\n",
       " Row(Age=49, Gender='Female', SleepHours=8.199999809265137, PhysicalActivity=2.299999952316284, AlcoholConsumption=4, Smoking='Yes', FamilyHistory='No', StressLevel=1, HeartRate=98, SweatingLevel=4, DietQuality=3, AnxietyLevel=1)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here using limit because collect returns all the record in a dataframe. Thast may cause memory error.\n",
    "df.limit(5).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a67741-34d4-4c79-aae8-f61f4072c396",
   "metadata": {},
   "source": [
    "***RDD(Resilient Distributed Dataset***\n",
    "- A Resilient Distributed Dataset (RDD), the basic abstraction in Spark. Represents an immutable, partitioned collection of elements that can be operated on in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9295f267-6781-4aba-a608-a84bfec0b70d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[15] at javaToPython at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  To see the Rdd\n",
    "df.rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6921b3a-02ad-4bd9-8189-b9b63d4a13a2",
   "metadata": {},
   "source": [
    "***df.take(n)***\n",
    "- Returns Number of first *n* rows of a dataframe.\n",
    "- docs: https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.take.html\n",
    "- df.head(n) also has similar functionalities.\n",
    "- df.tail(n) returns last n rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ae7d65e-f029-4831-b1b7-d8105c63f94d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([Row(Age=29, Gender='Female', SleepHours=6.0, PhysicalActivity=2.700000047683716, AlcoholConsumption=10, Smoking='Yes', FamilyHistory='No', StressLevel=10, HeartRate=114, SweatingLevel=4, DietQuality=7, AnxietyLevel=5)],\n",
       " [Row(Age=29, Gender='Female', SleepHours=6.0, PhysicalActivity=2.700000047683716, AlcoholConsumption=10, Smoking='Yes', FamilyHistory='No', StressLevel=10, HeartRate=114, SweatingLevel=4, DietQuality=7, AnxietyLevel=5)],\n",
       " [Row(Age=56, Gender='Other', SleepHours=6.099999904632568, PhysicalActivity=1.100000023841858, AlcoholConsumption=11, Smoking='No', FamilyHistory='No', StressLevel=1, HeartRate=66, SweatingLevel=3, DietQuality=8, AnxietyLevel=2)])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(1), df.head(1), df.tail(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa78d168-5f24-4401-938a-14d05451b455",
   "metadata": {},
   "source": [
    "***Selecting Multiple columns in a dataframe:***\n",
    "- df.select([çol1, col2, ....])\n",
    "- docs: https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.select.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2fab984-3574-4237-9218-3935b17755da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------------------+-------+\n",
      "|Age|Gender|AlcoholConsumption|Smoking|\n",
      "+---+------+------------------+-------+\n",
      "| 29|Female|                10|    Yes|\n",
      "| 46| Other|                 8|    Yes|\n",
      "| 64|  Male|                 4|     No|\n",
      "| 20|Female|                 6|    Yes|\n",
      "| 49|Female|                 4|    Yes|\n",
      "| 53|  Male|                 2|     No|\n",
      "| 20|  Male|                14|    Yes|\n",
      "| 54|Female|                15|     No|\n",
      "| 51| Other|                 2|     No|\n",
      "| 59|Female|                15|    Yes|\n",
      "| 30|  Male|                 4|     No|\n",
      "| 38| Other|                14|    Yes|\n",
      "| 45| Other|                 2|     No|\n",
      "| 31|Female|                 8|     No|\n",
      "| 31| Other|                 8|    Yes|\n",
      "| 44|Female|                16|     No|\n",
      "| 56| Other|                11|     No|\n",
      "| 61| Other|                 3|    Yes|\n",
      "| 57|Female|                 5|     No|\n",
      "| 29|Female|                 9|    Yes|\n",
      "+---+------+------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(['Age', 'Gender', 'AlcoholConsumption', 'Smoking']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0eab5460-e6e6-41a7-8b8a-8886606aa151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+----------------+------------------+-------+-------------+-----------+---------+-------------+-----------+------------+\n",
      "|Age|Gender|SleepHours|PhysicalActivity|AlcoholConsumption|Smoking|FamilyHistory|StressLevel|HeartRate|SweatingLevel|DietQuality|AnxietyLevel|\n",
      "+---+------+----------+----------------+------------------+-------+-------------+-----------+---------+-------------+-----------+------------+\n",
      "| 54|Female|       6.3|             5.5|                15|     No|           No|          5|      113|            1|          7|           4|\n",
      "| 59|Female|       5.1|             4.8|                15|    Yes|           No|          5|       95|            5|          1|           4|\n",
      "| 44|Female|       7.7|             2.0|                16|     No|          Yes|          3|       92|            3|          3|           2|\n",
      "| 34|Female|       4.5|             3.0|                11|     No|           No|          8|       82|            2|         10|           7|\n",
      "| 63|Female|       5.6|             2.5|                16|     No|          Yes|          4|       69|            4|         10|           2|\n",
      "| 34|Female|       4.6|             0.1|                10|    Yes|           No|          9|      112|            4|          4|           8|\n",
      "| 41|Female|       7.0|             3.5|                13|     No|          Yes|          1|       92|            1|          1|           2|\n",
      "| 51|Female|       7.9|             5.1|                12|    Yes|           No|          7|      113|            1|          7|           2|\n",
      "| 36|Female|       6.9|             4.6|                18|    Yes|           No|          7|       76|            1|          7|           4|\n",
      "| 60|Female|       7.2|             0.4|                15|     No|           No|          6|      116|            1|          3|           2|\n",
      "| 47|Female|       4.1|             2.6|                15|     No|           No|          2|       70|            3|          6|           2|\n",
      "| 34|Female|       6.4|             4.1|                14|     No|           No|         10|       96|            5|          2|           4|\n",
      "| 35|Female|       6.7|             2.3|                19|    Yes|          Yes|         10|       72|            3|          7|           4|\n",
      "| 43|Female|       6.4|             3.4|                18|     No|           No|          4|       80|            1|          2|           3|\n",
      "| 54|Female|       7.0|             1.5|                16|     No|           No|          9|      103|            5|          2|           3|\n",
      "| 48|Female|       4.1|             1.1|                16|    Yes|          Yes|          8|      118|            5|          4|          10|\n",
      "| 32|Female|       5.6|             1.9|                13|     No|          Yes|          2|       71|            4|          5|           3|\n",
      "| 32|Female|       7.6|             0.0|                16|     No|          Yes|          4|       84|            3|          3|           3|\n",
      "| 55|Female|       6.8|             2.5|                19|    Yes|          Yes|          8|      107|            4|          9|           4|\n",
      "| 34|Female|       5.8|             1.1|                14|    Yes|          Yes|          3|      104|            5|          3|           3|\n",
      "+---+------+----------+----------------+------------------+-------+-------------+-----------+---------+-------------+-----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fiter data where age greater than 30, Gender Female and alcohol consumption greater than 10\n",
    "\n",
    "df.filter(\n",
    "    (df.Age >= 30) &\n",
    "    (df.Gender == 'Female') &\n",
    "    (df.AlcoholConsumption >= 10)\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3dbb194-52c2-4b99-9dea-2b0ecd1e93d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-------+-----------+------------+\n",
      "|Age|Gender|Smoking|DietQuality|AnxietyLevel|\n",
      "+---+------+-------+-----------+------------+\n",
      "| 20| Other|     No|         10|           7|\n",
      "| 21|Female|     No|          9|           7|\n",
      "| 21|Female|     No|          9|           7|\n",
      "| 22| Other|     No|         10|           7|\n",
      "| 24|Female|     No|          9|           7|\n",
      "| 29|Female|     No|          9|           7|\n",
      "| 30|Female|     No|          9|           7|\n",
      "| 32| Other|     No|          8|           7|\n",
      "| 32|Female|     No|         10|           7|\n",
      "| 33|Female|     No|          9|           7|\n",
      "| 34|Female|     No|         10|           7|\n",
      "| 38|  Male|     No|          7|           7|\n",
      "| 39|Female|     No|          7|           7|\n",
      "| 40| Other|     No|         10|           7|\n",
      "| 40| Other|     No|         10|           8|\n",
      "| 42|Female|     No|          6|           7|\n",
      "| 43|Female|     No|         10|           7|\n",
      "| 46|Female|     No|          7|           7|\n",
      "| 46| Other|     No|          7|           7|\n",
      "| 46|Female|     No|          7|           7|\n",
      "+---+------+-------+-----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter data smoking is NO, diet quality is greater than 6 and anxiety level greater than 6 and select only Age, Gender, Smoking, DietQuality and AnxietyLevel order by Age Ascending\n",
    "df.filter(\n",
    "    (df.Smoking=='No') &\n",
    "    (df.DietQuality >= 6) &\n",
    "    (df.AnxietyLevel > 6)\n",
    ").select(['Age', 'Gender', 'Smoking', 'DietQuality', 'AnxietyLevel']).orderBy(df.Age.asc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "797b566b-bb23-45ff-a12a-9fdfe47838bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-------+-----------+------------+---------+\n",
      "|Age|Gender|Smoking|DietQuality|AnxietyLevel|GenderMap|\n",
      "+---+------+-------+-----------+------------+---------+\n",
      "| 29|Female|    Yes|          7|           5|        0|\n",
      "| 46| Other|    Yes|          8|           3|       -1|\n",
      "| 64|  Male|     No|          1|           1|        1|\n",
      "| 20|Female|    Yes|          1|           2|        0|\n",
      "| 49|Female|    Yes|          3|           1|        0|\n",
      "| 53|  Male|     No|          5|           4|        1|\n",
      "| 20|  Male|    Yes|          2|           4|        1|\n",
      "| 54|Female|     No|          7|           4|        0|\n",
      "| 51| Other|     No|          8|           3|       -1|\n",
      "| 59|Female|    Yes|          1|           4|        0|\n",
      "| 30|  Male|     No|          9|           2|        1|\n",
      "| 38| Other|    Yes|          3|           2|       -1|\n",
      "| 45| Other|     No|         10|           3|       -1|\n",
      "| 31|Female|     No|          4|           4|        0|\n",
      "| 31| Other|    Yes|         10|           4|       -1|\n",
      "| 44|Female|     No|          3|           2|        0|\n",
      "| 56| Other|     No|          3|           4|       -1|\n",
      "| 61| Other|    Yes|          3|           1|       -1|\n",
      "| 57|Female|     No|          4|           3|        0|\n",
      "| 29|Female|    Yes|          4|           9|        0|\n",
      "+---+------+-------+-----------+------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To Add a new column to the dataframe called Gender Map where 1 where Gender is Male , 0 where Gender is Female and -1 where Gender is Others\n",
    "from pyspark.sql.functions import when, col\n",
    "# condition = 1 if col('Gender') == 'Male' else 0 if col('Gender') == 'Female' else -1\n",
    "df.withColumn(\n",
    "    'GenderMap', when(df.Gender== 'Male', 1).when(df.Gender=='Female', 0).when(df.Gender=='Other', -1).otherwise(1)\n",
    ").select(['Age', 'Gender', 'Smoking', 'DietQuality', 'AnxietyLevel', 'GenderMap']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac907791-38c0-4640-9203-872f3f450ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+------------------+-------+-------------+-----------+---------+-------------+-----------+------------+\n",
      "|SleepHours|PhysicalActivity|AlcoholConsumption|Smoking|FamilyHistory|StressLevel|HeartRate|SweatingLevel|DietQuality|AnxietyLevel|\n",
      "+----------+----------------+------------------+-------+-------------+-----------+---------+-------------+-----------+------------+\n",
      "|       6.0|             2.7|                10|    Yes|           No|         10|      114|            4|          7|           5|\n",
      "|       6.2|             5.7|                 8|    Yes|          Yes|          1|       62|            2|          8|           3|\n",
      "+----------+----------------+------------------+-------+-------------+-----------+---------+-------------+-----------+------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dropping as column\n",
    "df.drop('Age', 'Gender').show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bdbf5664-d86c-4e7b-8216-e98a4eaae363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+----------------+------------------+-------+-------------+-----------+---------+-------------+-----------+------------+\n",
      "|Age|GENDER|SleepHours|PhysicalActivity|AlcoholConsumption|Smoking|FamilyHistory|StressLevel|HeartRate|SweatingLevel|DietQuality|AnxietyLevel|\n",
      "+---+------+----------+----------------+------------------+-------+-------------+-----------+---------+-------------+-----------+------------+\n",
      "| 29|Female|       6.0|             2.7|                10|    Yes|           No|         10|      114|            4|          7|           5|\n",
      "| 46| Other|       6.2|             5.7|                 8|    Yes|          Yes|          1|       62|            2|          8|           3|\n",
      "+---+------+----------+----------------+------------------+-------+-------------+-----------+---------+-------------+-----------+------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Renaing a column\n",
    "df.withColumnRenamed('Gender', 'GENDER').show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87e9434b-865f-4db2-9167-4efacfcb6ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Pyspark Userdefind Functions ♂, ♀\n",
    "# this will work with Spark\n",
    "# from pyspark.sql.functions import udf\n",
    "\n",
    "# @udf('string')\n",
    "# def make_symbol(gender):\n",
    "#     if gender == 'Male':\n",
    "#         return '♂'\n",
    "#     elif gender == 'female':\n",
    "#         return '♀'\n",
    "#     else:\n",
    "#         return gender\n",
    "\n",
    "# df.select('Age', 'Gender', make_symbol(df.Gender).alias('GenderIcon')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f56ca450-ec71-46ed-a5de-cf44dddb67ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o160.saveAsTextFile.\n: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/D:/codes/pyspark/RDD/anxiety already exists\r\n\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:131)\r\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.assertConf(SparkHadoopWriter.scala:299)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:71)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:965)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:963)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1623)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1623)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1609)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1609)\r\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile(JavaRDDLike.scala:564)\r\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile$(JavaRDDLike.scala:563)\r\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Save dataframe in an RDD in form of a text file\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# on windows it gives error\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrdd\u001b[49m\u001b[43m.\u001b[49m\u001b[43msaveAsTextFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m./RDD/anxiety\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompressionCodecClass\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\codes\\pyspark\\env\\Lib\\site-packages\\pyspark\\rdd.py:3425\u001b[39m, in \u001b[36mRDD.saveAsTextFile\u001b[39m\u001b[34m(self, path, compressionCodecClass)\u001b[39m\n\u001b[32m   3423\u001b[39m     keyed._jrdd.map(\u001b[38;5;28mself\u001b[39m.ctx._jvm.BytesToString()).saveAsTextFile(path, compressionCodec)\n\u001b[32m   3424\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3425\u001b[39m     \u001b[43mkeyed\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jrdd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jvm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mBytesToString\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msaveAsTextFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\codes\\pyspark\\env\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\codes\\pyspark\\env\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdeco\u001b[39m(*a: Any, **kw: Any) -> Any:\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    181\u001b[39m         converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\codes\\pyspark\\env\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    324\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    327\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    328\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    330\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    331\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    332\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling o160.saveAsTextFile.\n: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/D:/codes/pyspark/RDD/anxiety already exists\r\n\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:131)\r\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.assertConf(SparkHadoopWriter.scala:299)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:71)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:965)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:963)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1623)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1623)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1609)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1609)\r\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile(JavaRDDLike.scala:564)\r\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile$(JavaRDDLike.scala:563)\r\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n"
     ]
    }
   ],
   "source": [
    "# Save dataframe in an RDD in form of a text file\n",
    "# on windows it gives error\n",
    "# df.rdd.saveAsTextFile(path='./RDD/anxiety', compressionCodecClass=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d52d05-d1e2-4854-bc9d-7bc404cbe73f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
